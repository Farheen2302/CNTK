# steps 1
# generate LSTM LM on news comments

# -----------------------------
# preparing data
# data is also saved at
# \\speechstore5\transient\kaishengy\data\newscomments\2015\03-23
# -----------------------------
# first have </s> to the begining and ending of sentences
# go to ../scripts directory
python
>>> import util
>>> util.add_silence_ending('//speechstore5/transient/kaishengy/data/newscomments/2015/03-23/comments.txt', '//speechstore5/transient/kaishengy/data/newscomments/2015/03-23/comments.cntk.txt')
# the output sentences look like
# </s> god speed gentlemen . </s>

# convert data to ASCII format
>>> util.convert2ascii('//speechstore5/transient/kaishengy/data/newscomments/2015/03-23/comments.cntk.txt', '//speechstore5/transient/kaishengy/data/newscomments/2015/03-23/comments.cntk.ascii.txt')

# create test (last 10%), validation (first 10%) data, and train (remaining 80%) data
>>> util.split_data_into_train_valid_test('//speechstore5/transient/kaishengy/data/newscomments/2015/03-23/comments.cntk.ascii.txt', '//speechstore5/transient/kaishengy/data/newscomments/2015/03-23/comments.cntk.train.txt', '//speechstore5/transient/kaishengy/data/newscomments/2015/03-23/comments.cntk.valid.txt', '//speechstore5/transient/kaishengy/data/newscomments/2015/03-23/comments.cntk.test.txt')

# prepare source side data
>>> import util
>>> util.add_silence_ending('//speechstore5/transient/kaishengy/data/newscomments/2015/03-23/news.txt', '//speechstore5/transient/kaishengy/data/newscomments/2015/03-23/news.cntk.txt')
>>> util.add_silence_ending('d:/data/newscomments/2015/03-23/news.txt', 'd:/data/newscomments/2015/03-23/news.cntk.txt')
# the output sentences look like
# </s> god speed gentlemen . </s>

# convert data to ASCII format
>>> util.convert2ascii('//speechstore5/transient/kaishengy/data/newscomments/2015/03-23/news.cntk.txt', '//speechstore5/transient/kaishengy/data/newscomments/2015/03-23/news.cntk.ascii.txt')
>>> util.convert2ascii('d:/data/newscomments/2015/03-23/news.cntk.txt', 'd:/data/newscomments/2015/03-23/news.cntk.ascii.txt')

# create test (last 10%), validation (first 10%) data, and train (remaining 80%) data
>>> util.split_data_into_train_valid_test('//speechstore5/transient/kaishengy/data/newscomments/2015/03-23/news.cntk.ascii.txt', '//speechstore5/transient/kaishengy/data/newscomments/2015/03-23/news.cntk.train.txt', '//speechstore5/transient/kaishengy/data/newscomments/2015/03-23/news.cntk.valid.txt', '//speechstore5/transient/kaishengy/data/newscomments/2015/03-23/news.cntk.test.txt')
>>> util.split_data_into_train_valid_test('d:/data/newscomments/2015/03-23/news.cntk.ascii.txt', 'd:/data/newscomments/2015/03-23/news.cntk.train.txt', 'd:/data/newscomments/2015/03-23/news.cntk.valid.txt', 'd:/data/newscomments/2015/03-23/news.cntk.test.txt')


# -----------------------------
# create word cluster
# -----------------------------
# goto ../steps directory
# use cutoff = 2
mkdir \\speechstore5\transient\kaishengy\exp\news\s1.lstmlm.comments
D:\dev\cntkcodeplex\x64\Debug\CNTK.exe configFile=..\setups\global.config+..\setups\lstmlm.gpu.nce.config.txt ExpDir=\\speechstore5\transient\kaishengy\exp\news\s1.lstmlm.comments command=writeWordAndClassInfo
# check $ExpDir$ and $DataFolder$ for outputs

# -----------------------------
# Train LSTM LM using NCE criterion
# -----------------------------
# the vocabulary size is 2263
# in local
D:\dev\cntkcodeplex\x64\Release\cntk.exe configFile=..\setups\global.config+..\setups\lstmlm.gpu.nce.config.txt ExpDir=\\speechstore5\transient\kaishengy\exp\news\s1.lstmlm.comments command=train
# on server
\\speechstore5\userdata\kaishengy\bin\binmay26\cntk.exe configFile=\\speechstore5\userdata\kaishengy\exp\news\setups\global.config+\\speechstore5\userdata\kaishengy\exp\news\setups\lstmlm.gpu.nce.config.txt ExpDir=\\speechstore5\transient\kaishengy\exp\news\s1.lstmlm.comments command=train

# test PPL : 61.31 with 1 layer of 200 hidden dimension LSTM 
\\speechstore5\userdata\kaishengy\bin\binmay26\cntk.exe configFile=\\speechstore5\userdata\kaishengy\exp\news\setups\global.config+\\speechstore5\userdata\kaishengy\exp\news\setups\lstmlm.gpu.nce.config.txt ExpDir=\\speechstore5\transient\kaishengy\exp\news\s1.lstmlm.comments test=[modelPath=$ExpFolder$\modelRnnCNTK] command=test

# ---- 2 layers of LSTM --------
# in local wit 2 layers of LSTM
D:\dev\cntkcodeplex\x64\Release\cntk.exe configFile=..\setups\global.config+..\setups\lstmlm.gpu.nce.config.txt ExpDir=d:\exp\news\s1.lstmlm.comments.2layers command=train train=[SimpleNetworkBuilder=[layerSizes=$VOCABSIZE$:200:200:$VOCABSIZE$]] DataDir=d:\data\newscomments\2015\03-23

# on server
\\speechstore5\userdata\kaishengy\bin\binmay26\cntk.exe configFile=\\speechstore5\userdata\kaishengy\exp\news\setups\global.config+\\speechstore5\userdata\kaishengy\exp\news\setups\lstmlm.gpu.nce.config.txt ExpDir=\\speechstore5\transient\kaishengy\exp\news\s1.lstmlm.comments.2layers command=train train=[SimpleNetworkBuilder=[layerSizes=$VOCABSIZE$:200:200:$VOCABSIZE$]]
# test 
# PPL Perplexity = 36.1999
\\speechstore5\userdata\kaishengy\bin\binmay28\cntk.exe configFile=\\speechstore5\userdata\kaishengy\exp\news\setups\global.config+\\speechstore5\userdata\kaishengy\exp\news\setups\lstmlm.gpu.nce.config.txt ExpDir=\\speechstore5\transient\kaishengy\exp\news\s1.lstmlm.comments.2layers test=[modelPath=$ExpFolder$\modelRnnCNTK] command=test

# use 3 layers of LSTM 52.85
\\speechstore5\userdata\kaishengy\bin\binmay28\cntk.exe configFile=\\speechstore5\userdata\kaishengy\exp\news\setups\global.config+\\speechstore5\userdata\kaishengy\exp\news\setups\lstmlm.gpu.nce.config.txt ExpDir=\\speechstore5\transient\kaishengy\exp\news\s1.lstmlm.comments.3layers train=[SimpleNetworkBuilder=[layerSizes=$VOCABSIZE$:200:200:200:$VOCABSIZE$]] test=[modelPath=$ExpFolder$\modelRnnCNTK] DEVICE=0
